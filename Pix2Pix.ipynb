{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pix2Pix",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jdubkim/SAGAN-Keras/blob/master/Pix2Pix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "pEEsZeHJLmXY",
        "colab_type": "code",
        "outputId": "58d2355d-557a-457c-d981-20aefc2b588f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "!pip install tensorflow-gpu==2.0.0-alpha0\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "from tensorflow.keras.layers import Input, Dropout, Concatenate, Conv2D, UpSampling2D, LeakyReLU, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0.0-alpha0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/66/32cffad095253219d53f6b6c2a436637bbe45ac4e7be0244557210dc3918/tensorflow_gpu-2.0.0a0-cp36-cp36m-manylinux1_x86_64.whl (332.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 332.1MB 65kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.11.0)\n",
            "Collecting tb-nightly<1.14.0a20190302,>=1.14.0a20190301 (from tensorflow-gpu==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/51/aa1d756644bf4624c03844115e4ac4058eff77acd786b26315f051a4b195/tb_nightly-1.14.0a20190301-py3-none-any.whl (3.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.0MB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.7)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.2.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.9)\n",
            "Collecting tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 (from tensorflow-gpu==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/82/f16063b4eed210dc2ab057930ac1da4fbe1e91b7b051a6c8370b401e6ae7/tf_estimator_nightly-1.14.0.dev2019030115-py2.py3-none-any.whl (411kB)\n",
            "\u001b[K    100% |████████████████████████████████| 419kB 12.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n",
            "Collecting google-pasta>=0.1.2 (from tensorflow-gpu==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/bb/f1bbc131d6294baa6085a222d29abadd012696b73dcbf8cf1bf56b9f082a/google_pasta-0.1.5-py3-none-any.whl (51kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 23.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (3.7.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.33.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.16.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (3.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (0.15.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-alpha0) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-alpha0) (40.9.0)\n",
            "Installing collected packages: tb-nightly, tf-estimator-nightly, google-pasta, tensorflow-gpu\n",
            "Successfully installed google-pasta-0.1.5 tb-nightly-1.14.0a20190301 tensorflow-gpu-2.0.0a0 tf-estimator-nightly-1.14.0.dev2019030115\n",
            "2.0.0-alpha0\n",
            "2.0.0-alpha0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q_SBwTekMasR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "38447767-14c6-49a3-b418-7d1ae79c93ec"
      },
      "cell_type": "code",
      "source": [
        "_URL = 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz'\n",
        "\n",
        "path_to_zip = tf.keras.utils.get_file('facades.tar.gz',\n",
        "                                      origin=_URL,  \n",
        "                                      extract=True)\n",
        "\n",
        "PATH = os.path.join(os.path.dirname(path_to_zip), 'facades/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz\n",
            "30171136/30168306 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oedulLUjMtNq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def parse_args():\n",
        "  desc = \"Keras Implementation of Self-Attention GAN\"\n",
        "  \n",
        "  parser = argparse.ArgumentParser(desc)\n",
        "\n",
        "  parser.add_argument('--phase', type=str, default='train')\n",
        "  parser.add_argument('--dataset', type=str, default='facades', help='mnist | cifar10')\n",
        "\n",
        "  parser.add_argument('--epoch', type=int, default=100, help='The number of epochs to run')\n",
        "  parser.add_argument('--iteration', type=int, default=10000, help='The number of training iterations')\n",
        "  parser.add_argument('--print_freq', type=int, default=500, help='The number of image_print_freqy')\n",
        "  parser.add_argument('--save_freq', type=int, default=500, help='The number of ckpt_save_freq')\n",
        "\n",
        "  parser.add_argument('--g_lr', type=float, default=0.0001, help='learning rate for generator')\n",
        "  parser.add_argument('--d_lr', type=float, default=0.0004, help='learning rate for discriminator')\n",
        "  parser.add_argument('--beta1', type=float, default=0.0, help='beta1 for Adam optimizer')\n",
        "  parser.add_argument('--beta2', type=float, default=0.9, help='beta2 for Adam optimizer')\n",
        "\n",
        "  parser.add_argument('--z_dim', type=int, default=128, help='Dimension of noise vector')\n",
        "  parser.add_argument('--up_sample', type=bool, default=True, help='using upsample-conv')\n",
        "  parser.add_argument('--sn', type=bool, default=True, help='using spectral norm')\n",
        "  parser.add_argument('--ld', type=float, default=10.0, help='gradient penalty lambda')\n",
        "  parser.add_argument('--n_critic', type=int, default=1, help='The number of critic')\n",
        "\n",
        "  parser.add_argument('--img_shape', type=tuple, default=(256, 256, 3), help='The size of image')\n",
        "  parser.add_argument('--sample_num', type=int, default=64, help='The number of sample images')\n",
        "\n",
        "  parser.add_argument('--test_num', type=int, default=10, help='The number of images generated by the test')\n",
        "\n",
        "  parser.add_argument('--checkpoint_dir', type=str, default='checkpoint', help='Directory name to save checkpoints')\n",
        "  parser.add_argument('--result_dir', type=str, default='results', help='Directory name to save generated images')\n",
        "  parser.add_argument('--log_dir', type=str, default='logs', help='Directory name to save training logs')\n",
        "  parser.add_argument('--sample_dir', type=str, default='samples', help='Directory name to save samples on training')\n",
        "\n",
        "  return parser.parse_args()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xwWNjktLLynG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Pix2Pix():\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.img_rows = 256\n",
        "    self.img_cols = 256\n",
        "    self.channels = 3\n",
        "    self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "    \n",
        "    # self.dataset = dataset # dataset name\n",
        "    self.dataset_name = 'facades'\n",
        "    self.data_loader = DataLoader(dataset_name=self.dataset_name,\n",
        "                                 img_res=(self.img_rows, self.img_cols))\n",
        "    \n",
        "    patch = int(self.img_rows / 2**4)\n",
        "    self.disc_patch = (patch, patch, 1)\n",
        "    self.gf = 64\n",
        "    self.df = 64\n",
        "    \n",
        "    optimizer = Adam(0.0002, 0.5)\n",
        "    \n",
        "    self.discriminator = self.Discriminator()\n",
        "    self.discriminator.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n",
        "    \n",
        "    self.generator = self.Generator()\n",
        "    \n",
        "    img_A = Input(shape=self.img_shape)\n",
        "    img_B = Input(shape=self.img_shape)    \n",
        "    \n",
        "    fake_A = self.generator(img_B)\n",
        "    \n",
        "    self.discriminator.trainable = False\n",
        "    \n",
        "    valid = self.discriminator([fake_A, img_B])\n",
        "    \n",
        "    self.combined = Model(inputs=[img_A, img_B], outputs=[valid, fake_A])\n",
        "    \n",
        "    self.combined.compile(loss=['mse', 'mae'],\n",
        "                         loss_weights=[1, 100],\n",
        "                         optimizer=optimizer)\n",
        "    \n",
        "  \n",
        "  def Generator(self):\n",
        "    \n",
        "    def conv2d(layer_input, filters, f_size=4, bn=True):\n",
        "      d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
        "      d = LeakyReLU(alpha=0.2)(d)\n",
        "      if bn:\n",
        "        d = BatchNormalization(momentum=0.8)(d)\n",
        "        return d\n",
        "          \n",
        "    \n",
        "    def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
        "      \"\"\"Layers used during upsampling\"\"\"\n",
        "      u = UpSampling2D(size=2)(layer_input)\n",
        "      u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
        "      if dropout_rate:\n",
        "        u = Dropout(dropout_rate)(u)\n",
        "      u = BatchNormalization(momentum=0.8)(u)\n",
        "      u = Concatenate()([u, skip_input])\n",
        "       \n",
        "      return u\n",
        "          \n",
        "    d0 = Input(shape=self.img_shape)\n",
        "    \n",
        "    # Downsampling\n",
        "    d1 = conv2d(d0, self.gf, bn=False)\n",
        "    d2 = conv2d(d1, self.gf*2)\n",
        "    d3 = conv2d(d2, self.gf*4)\n",
        "    d4 = conv2d(d3, self.gf*8)\n",
        "    d5 = conv2d(d4, self.gf*8)\n",
        "    d6 = conv2d(d5, self.gf*8)\n",
        "    d7 = conv2d(d6, self.gf*8)\n",
        "    \n",
        "    # Upsampling\n",
        "    \n",
        "    u1 = deconv2d(d7, d6, self.gf*8)\n",
        "    u2 = deconv2d(u1, d5, self.gf*8)\n",
        "    u3 = deconv2d(u2, d4, self.gf*8)\n",
        "    u4 = deconv2d(u3, d3, self.gf*4)\n",
        "    u5 = deconv2d(u4, d2, self.gf*2)\n",
        "    u6 = deconv2d(u5, d1, self.gf)\n",
        "    \n",
        "    u7 = UpSampling2D(size=2)(u6)\n",
        "    \n",
        "    output_img = Conv2D(self.channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u7)\n",
        "    \n",
        "    return tf.keras.Model(d0, output_img)\n",
        "  \n",
        "  def Discriminator(self):\n",
        "    \n",
        "    def d_layer(layer_input, filters, f_size=4, bn=True):\n",
        "      \n",
        "      d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
        "      d = LeakyReLU(alpha=0.2)(d)\n",
        "      if bn:\n",
        "        d = BatchNormalization(momentum=0.8)(d)\n",
        "      return d\n",
        "    \n",
        "    img_A = Input(shape=self.img_shape)\n",
        "    img_B = Input(shape=self.img_shape)\n",
        "    \n",
        "    combined_imgs = Concatenate(axis=-1)([img_A, img_B])\n",
        "    \n",
        "    d1 = d_layer(combined_imgs, self.df, bn=False)\n",
        "    d2 = d_layer(d1, self.df*2)\n",
        "    d3 = d_layer(d2, self.df*4)\n",
        "    d4 = d_layer(d3, self.df*8)\n",
        "    \n",
        "    validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
        "    \n",
        "    return tf.keras.Model([img_A, img_B], validity)\n",
        "  \n",
        "  def train(self, epochs, batch_size=1, sample_interval=50):\n",
        "    \n",
        "    valid = np.ones((batch_size, ) + self.disc_patch)\n",
        "    fake = np.zeros((batch_size, ) + self.disc_patch)\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "      for batch_i, (imgs_A, imgs_B) in enumerate(self.data_loader.load_batch(batch_size)):\n",
        "        \n",
        "        # Train Discriminator\n",
        "        \n",
        "        fake_A = self.generator(imgs_B)\n",
        "        \n",
        "        d_loss_real = self.discriminator.train_on_batch([imgs_A, imgs_B], valid)\n",
        "        d_loss_fake = self.discriminator.train_on_batch([fake_A, imgs_B], fake)\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "        \n",
        "        # Train Generator\n",
        "        \n",
        "        g_loss = self.combined.train_on_batch([imgs_A, imgs_B], [valid, imgs_A])\n",
        "        \n",
        "        print(\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %f]\" % \n",
        "             (epoch, epochs, batch_i, self.data_loader.n_batches, d_loss[0], 100*d_loss[1],\n",
        "             g_loss[0]))\n",
        "        \n",
        "        if batch_i % sample_interval == 0:\n",
        "          self.sample_images(epoch, batch_i)\n",
        "          \n",
        "  def sample_images(self, epoch, batch_i):\n",
        "    \n",
        "    if patch_check:\n",
        "      os.makedirs('images/%s' % self.dataset_name, exist_ok=True)\n",
        "     \n",
        "    r, c = 3, 3\n",
        "    \n",
        "    imgs_A, imgs_B = self.data_loader.load_data(batch_size=3, is_testing=True)\n",
        "    fake_A = self.generator.predict(imgs_B)\n",
        "    \n",
        "    gen_imgs = np.concatenate([imgs_B, fake_A, imgs_A])\n",
        "    \n",
        "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "    \n",
        "    titles = ['Condition', 'Generated', 'Original']\n",
        "    fig, axs = plt.subplots(r, c)\n",
        "    cnt = 0\n",
        "    for i in range(r):\n",
        "      for j in range(c):\n",
        "        axs[i, j].imshow(gen_imgs[cnt])\n",
        "        axs[i, j].set_title(titles[i])\n",
        "        axs[i, j].axis('off')\n",
        "        cnt += 1\n",
        "    \n",
        "    fig.savefig(\"images/%s/%d_%d.png\" % (self.dataset_name, epoch, batch_i))\n",
        "    plt.close()    \n",
        "    \n",
        "        \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fK73UABXpaKZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def download_dataset(dataset_name, URL):\n",
        "  \n",
        "  # check if we have dataset or not\n",
        "  if not os.path.exists('./' + dataset_name):\n",
        "    path_to_zip = tf.keras.utils.get_file('facades.tar.gz',\n",
        "                                      origin=_URL,  \n",
        "                                      extract=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k8PiZmtLjBdY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DataLoader():\n",
        "  def __init__(self, img_shape, PATH):\n",
        "    self.width = img_shape[0]\n",
        "    self.height = img_shape[1]\n",
        "    self.PATH = PATH\n",
        "    \n",
        "  def load_img(self, img_file):\n",
        "    img = tf.io.read_file(img_file)\n",
        "    img = tf.image.decode_jpeg(img)\n",
        "    \n",
        "    w = tf.shape(img)[1]\n",
        "    \n",
        "    w = w // 2\n",
        "    real_img = img[:, :w, :]\n",
        "    input_img = img[:, w:, :]\n",
        "    \n",
        "    real_img = tf.cast(real_img, tf.float32)\n",
        "    input_img = tf.cast(input_img, tf.float32)\n",
        "    \n",
        "    return input_img, real_img\n",
        "    \n",
        "  def resize(self, input_img, real_img, height, width):\n",
        "    input_img = tf.image.resize(input_img, [height, width], \n",
        "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "    real_img = tf.image.resize(real_img, [height, width], \n",
        "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "    \n",
        "    return input_img, real_img\n",
        "  \n",
        "  def random_crop(self, input_img, real_img):\n",
        "    stacked_img = tf.stack([input_img, real_img], axis=0)\n",
        "    cropped_img = tf.image.random_crop(\n",
        "        stacked_img, size=[2, self.height, self.width, 3])\n",
        "    \n",
        "    return cropped_img[0], cropped_img[1]\n",
        "  \n",
        "  def normalize(self, input_img, real_img):\n",
        "    input_img = (input_img / 127.5) - 1\n",
        "    real_img = (real_img / 127.5) - 1\n",
        "    \n",
        "    return input_img, real_img\n",
        "  \n",
        "  def random_jitter(self, input_img, real_img):\n",
        "    # resizing to 286 x 286 x 3\n",
        "    input_img, real_img = self.resize(input_img, real_img, 286, 286)\n",
        "    \n",
        "    input_img, real_img = self.random_crop(input_img, real_img)\n",
        "    \n",
        "    if tf.random.uniform(()) > 0.5:\n",
        "      input_img = tf.image.flip_left_right(input_img)\n",
        "      \n",
        "  \n",
        "  def load_img_train(self, img_file):\n",
        "    input_img, real_img = self.load(img_file)\n",
        "    input_img, real_img = self.random_jitter(input_img, real_img)\n",
        "    input_img, real_img = self.normalize(input_img, real_img)\n",
        "    \n",
        "    return input_img, real_img\n",
        "  \n",
        "  def load_img_test(self, img_file):\n",
        "    input_img, real_img = self.load(img_file)\n",
        "    input_img, real_img = self.resize(input_img, real_img, self.height, self.width)\n",
        "    input_img, real_img = self.normalize(input_img, real_img)\n",
        "    \n",
        "    return input_img, real_img\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "huJRh7xhO74B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "888e7f28-c289-4346-8a6e-3b6ad791c973"
      },
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  #args = parse_args()\n",
        "  data_loaer = DataLoader((256, 256, 3), _URL)\n",
        "  pix2pix = Pix2Pix()\n",
        "  pix2pix.train(epochs=200, batch_size=1, sample_interval=200)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-771e4dd4f1a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;31m#args = parse_args()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mdata_loaer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_URL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mpix2pix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPix2Pix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mpix2pix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-86adf19b9b51>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'facades'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     self.data_loader = DataLoader(dataset_name=self.dataset_name,\n\u001b[0;32m---> 12\u001b[0;31m                                  img_res=(self.img_rows, self.img_cols))\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mpatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_rows\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'dataset_name'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "dnpt9lDKj6sp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}